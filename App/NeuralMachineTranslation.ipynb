{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EZg7sHZh7fAD"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MwcEkPpA7j2o"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"data (1).zip\", 'r') as zObject:\n",
    "    zObject.extractall(path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "A2dHKMKm8AMS"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.word2index = {}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "    self.n_words = 2\n",
    "  def addSentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.addWord(word)\n",
    "  def addWord(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v-TYWyLI9vUj"
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "  return ''.join(\n",
    "      c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn'\n",
    "  )\n",
    "\n",
    "def normalizeString(s):\n",
    "  s = unicodeToAscii(s.lower().strip())\n",
    "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "  return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Zk9gnnox-LzA"
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "  print(\"Reading Lines......\")\n",
    "\n",
    "  lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "  if reverse:\n",
    "    pairs = [list(reversed(p)) for p in pairs]\n",
    "    input_lang = Lang(lang2)\n",
    "    output_lang = Lang(lang1)\n",
    "  else:\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "  return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FUf5p6Dh_xPR"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "  return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "  return [p for p in pairs if filterPair(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8bJ-FR9G5VA",
    "outputId": "a8b98ece-829f-4077-ded7-30dbf73ce10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "  print(\"Read %s sentence pairs\" %len(pairs))\n",
    "  pairs = filterPairs(pairs)\n",
    "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "  print(\"Counting words....\")\n",
    "  for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "  print(\"Counted Words\")\n",
    "  print(input_lang.name, input_lang.n_words)\n",
    "  print(output_lang.name, output_lang.n_words)\n",
    "  return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng','fra',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOw1kLlcH462",
    "outputId": "0d4f0ea6-aeaa-4fca-d67f-02b095872a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je suis en cours de concentration', 'i m concentrating']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XXWnxwLRIAqA"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input):\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    # print('embedded :', embedded.shape)\n",
    "    output, hidden = self.gru(embedded)\n",
    "    # print('output :', output.shape)\n",
    "    # print('Hidden :', hidden[0].shape)\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ko1DAxupMVH2"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "  def __init__(self, hidden_size):\n",
    "    super(BahdanauAttention,self).__init__()\n",
    "    self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "    self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "  def forward(self, query, keys):\n",
    "    # print(self.Wa(query).shape)\n",
    "    # print(self.Ua(keys).shape)\n",
    "    scores = self.Va(torch.tanh( self.Wa(query) + self.Ua(keys) ))\n",
    "    # print('Scores :', scores.shape)\n",
    "    scores = scores.squeeze(2).unsqueeze(1)\n",
    "    # print('Scores :', scores.shape)\n",
    "\n",
    "    weights = F.softmax(scores, dim = -1)\n",
    "    # print('Weights : ', weights.shape)\n",
    "    context = torch.bmm(weights, keys)\n",
    "    # print('context : ', context.shape)\n",
    "    return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "p3zHlFYaOBH2"
   },
   "outputs": [],
   "source": [
    "# att = BahdanauAttention(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "AOPhdY6BOHN7"
   },
   "outputs": [],
   "source": [
    "# query = torch.randn((32, 1 ,128))\n",
    "# keys = torch.randn((32, 10, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "6nQfa7NrOAN_"
   },
   "outputs": [],
   "source": [
    "# context, weights = att.forward(query, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9KhlRxfTOWz5"
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    self.attention = BahdanauAttention(hidden_size)\n",
    "    self.gru = nn.GRU(2*hidden_size, hidden_size, batch_first=True)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "    batch_size = encoder_outputs.size(0)\n",
    "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "    # print('initial_decoder_inp : ', decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # print('decoder_hidden:', decoder_hidden.shape)\n",
    "    decoder_outputs = []\n",
    "    attentions = []\n",
    "    for i in range(MAX_LEN):\n",
    "      decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "          decoder_input, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "      decoder_outputs.append(decoder_output)\n",
    "      attentions.append(attn_weights)\n",
    "\n",
    "      if target_tensor is not None:\n",
    "        decoder_input = target_tensor[:,i].unsqueeze(1)\n",
    "\n",
    "      else:\n",
    "        _, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "    attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "    return decoder_outputs, decoder_hidden, attentions\n",
    "  def forward_step(self, inp, hidden, encoder_outputs):\n",
    "    # print('inp : ', inp.shape)\n",
    "    embedded = self.dropout(self.embedding(inp))\n",
    "    # print('hidden : ', hidden.shape)\n",
    "    query = hidden.permute(1,0,2)\n",
    "    # print('query : ', query.shape)\n",
    "    # print('enc_out : ', encoder_outputs.shape)\n",
    "    context, attn_weights = self.attention(query, encoder_outputs)\n",
    "    input_gru = torch.cat((embedded, context), dim=2)\n",
    "    # print('input_gru :', input_gru.shape)\n",
    "    output, hidden  = self.gru(input_gru, hidden)\n",
    "    output = self.out(output)\n",
    "\n",
    "    return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "cDqypEjfS6fN"
   },
   "outputs": [],
   "source": [
    "enc = EncoderRNN(input_lang.n_words, hidden_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "57RpDVHRS8qp"
   },
   "outputs": [],
   "source": [
    "rand_inp = torch.randint(1, input_lang.n_words, size=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YBxlqtypS-XL",
    "outputId": "cd131c7c-008b-4a81-9b52-5570544d2ff8"
   },
   "outputs": [],
   "source": [
    "output, hidden = enc.forward(rand_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "luFKC0FPSpSc"
   },
   "outputs": [],
   "source": [
    "# dec = AttnDecoderRNN(128, output_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "170o6zhTTHKJ"
   },
   "outputs": [],
   "source": [
    "# ran_tar = torch.randint(1, output_lang.n_words, size=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "GBR6rZLRSwNJ"
   },
   "outputs": [],
   "source": [
    "# out, hid, att = dec.forward(output, hidden, ran_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Uq78n3jDTExV"
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "  indexes = indexesFromSentence(lang, sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1,-1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "  return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "  input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "  n = len(pairs)\n",
    "  input_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "  target_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "\n",
    "  for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                             torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "  return input_lang, output_lang, train_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qzWJIenDqjxl"
   },
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "  total_loss = 0\n",
    "  for data in data_loader:\n",
    "    input_tensor, target_tensor = data\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs,_,_ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "  return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "psKLxkoBrk7a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "jK0ByxfUro09"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0\n",
    "  plot_loss_total = 0\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "KqIfxAqRtDD0"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "  with torch.no_grad():\n",
    "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    decoder_outputs, decoder_hidden, decoder_attention = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "\n",
    "    for idx in decoded_ids:\n",
    "      if idx.item()==EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "      else:\n",
    "        decoded_words.append(output_lang.index2word[idx.item()])\n",
    "  return decoded_words, decoder_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "c-lGmv_MuMZ1"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aGa-bUbuNCk",
    "outputId": "137a29c1-11da-4e2c-c912-3454070f6092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n",
      "2m 30s (- 37m 35s) (5 6%) 1.5117\n",
      "5m 2s (- 35m 19s) (10 12%) 0.6621\n",
      "7m 47s (- 33m 45s) (15 18%) 0.3432\n",
      "10m 28s (- 31m 26s) (20 25%) 0.1904\n",
      "13m 10s (- 28m 58s) (25 31%) 0.1180\n",
      "15m 57s (- 26m 36s) (30 37%) 0.0830\n",
      "18m 32s (- 23m 49s) (35 43%) 0.0637\n",
      "21m 3s (- 21m 3s) (40 50%) 0.0528\n",
      "23m 35s (- 18m 21s) (45 56%) 0.0453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m   loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m   print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(data_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m      7\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m encoder_outputs, encoder_hidden \u001b[38;5;241m=\u001b[39m encoder(input_tensor)\n\u001b[0;32m---> 10\u001b[0m decoder_outputs,_,_ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     12\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     13\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward\u001b[0;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001b[0m\n\u001b[1;32m     17\u001b[0m attentions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_LEN):\n\u001b[0;32m---> 19\u001b[0m   decoder_output, decoder_hidden, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m   decoder_outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n\u001b[1;32m     23\u001b[0m   attentions\u001b[38;5;241m.\u001b[39mappend(attn_weights)\n",
      "Cell \u001b[0;32mIn[41], line 47\u001b[0m, in \u001b[0;36mAttnDecoderRNN.forward_step\u001b[0;34m(self, inp, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m input_gru \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, context), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# print('input_gru :', input_gru.shape)\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m output, hidden  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(output)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, hidden, attn_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.9/site-packages/torch/nn/modules/rnn.py:1102\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1106\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlAdbK42uTw7",
    "outputId": "a84bdde0-11eb-4ecc-9253-2dec5bc9720c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> elle a loupe le coche\n",
      "= she s missed the boat\n",
      "< she s missed the boat of the other of the\n",
      "\n",
      "> c est un artiste celebre\n",
      "= he is a famous artist\n",
      "< he s a famous artist <EOS>\n",
      "\n",
      "> il est bon conducteur\n",
      "= he is good at driving\n",
      "< he is good at driving <EOS>\n",
      "\n",
      "> je n ai vraiment pas aussi faim que ca\n",
      "= i m really not all that hungry\n",
      "< i m really not all that hungry <EOS>\n",
      "\n",
      "> je suis votre avocat\n",
      "= i m your lawyer\n",
      "< i m your lawyer <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfIUSVVenO-8",
    "outputId": "591b5d00-19e4-4722-c22c-be70c4128c2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9mxqAqKnCxO",
    "outputId": "2eea84fa-8e8e-445f-aa39-4a934e7d208b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.28764198060873264\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [word_tokenize(sentence.lower()) for sentence in reference]\n",
    "    candidate = word_tokenize(candidate.lower())\n",
    "\n",
    "    smoothing_function = SmoothingFunction().method1  # You can choose different smoothing methods\n",
    "\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "\n",
    "# Example usage:\n",
    "reference_sentence = [\"The cat is sitting on the mat\"]\n",
    "candidate_sentence = \"The cat is on the mat\"\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vkye2saGnMfE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bleu_metric(encoder, decoder, n=10):\n",
    "    bleu=0\n",
    "    for i in tqdm(range(n)):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        reference_sentence = [pair[1]]\n",
    "        candidate_sentence = output_sentence\n",
    "        bleu+= calculate_bleu(reference_sentence, candidate_sentence)\n",
    "    return bleu/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6wkUVAvnOEN",
    "outputId": "60ad574d-9337-4eca-fa25-73fa83882475"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:11<00:00, 140.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42371017698449914"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric(encoder, decoder, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "4_5a7iVmnZLw"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"input_lang.pkl\",'wb') as f:\n",
    "    pickle.dump(input_lang, f)\n",
    "\n",
    "with open(\"output_lang.pkl\",'wb') as f:\n",
    "    pickle.dump(output_lang,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the encoder and decoder models\n",
    "torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
